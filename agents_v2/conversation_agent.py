from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage
from .state import AgentState
from .streaming import get_stream_manager
import yaml
import os
import logging

logger = logging.getLogger(__name__)

def load_prompt(filename: str) -> str:
    """Load prompt from YAML file."""
    try:
        prompt_path = os.path.join(os.path.dirname(__file__), "prompts", filename)
        with open(prompt_path, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
            return data.get('prompt', '')
    except Exception as e:
        logger.error(f"Failed to load prompt {filename}: {e}")
        return "Tu es un assistant conversationnel proactif et engageant."

CONVERSATION_PROMPT_TEMPLATE = load_prompt("conversation.yaml")


class ConversationAgent:
    """Agent conversationnel pour guider l'utilisateur de mani√®re proactive."""
    
    def __init__(self, state: AgentState):
        self.state = state
        self.project_id = state.get("project_id")
        self.tool_results = state.get("tool_results", {})
        self.current_agent = state.get("current_agent")
        self.error = state.get("error")
        
    async def suggest_next_steps(self) -> dict:
        """Sugg√©rer les prochaines actions bas√©es sur le contexte actuel.
        
        Returns:
            Dict avec:
            - suggestions: list[str] - Liste de suggestions d'actions
            - context: str - Contexte expliquant pourquoi ces suggestions
            - priority: str - high/medium/low
        """
        logger.info(f"ConversationAgent suggesting next steps based on context: {self.current_agent}")
        
        suggestions = []
        context = ""
        priority = "medium"
        
        # Cas 1: Apr√®s analyse de document
        if self.current_agent == "document":
            doc_results = self._extract_document_results()
            
            # Prioriser les features cr√©√©es sur les documents analys√©s
            if doc_results.get("features_created"):
                feature_ids = doc_results.get("features_created", [])
                suggestions = [
                    f"üìù G√©n√©rer les User Stories pour ces {len(feature_ids)} features",
                    "üéØ Ajouter des crit√®res d'acceptation d√©taill√©s",
                    "üèóÔ∏è Organiser sous des Epics th√©matiques"
                ]
                context = f"{len(feature_ids)} features cr√©√©es avec succ√®s !"
                priority = "high"
            
            elif doc_results.get("documents_analyzed"):
                analyzed_count = len(doc_results.get("documents_analyzed", []))
                suggestions = [
                    "üöÄ Extraire automatiquement les features de ces documents",
                    "üîç Rechercher des exigences sp√©cifiques",
                    "üìä G√©n√©rer un r√©sum√© structur√©"
                ]
                context = f"{analyzed_count} document{'s' if analyzed_count > 1 else ''} analys√©{'s' if analyzed_count > 1 else ''} - pr√™t pour l'extraction"
                priority = "high"
            
            else:
                suggestions = [
                    "üì§ Uploader un document (CDC, sp√©cifications)",
                    "üîé Analyser les documents du projet",
                    "üìÑ Consulter les documents disponibles"
                ]
                context = "Aucun document analys√© pour le moment"
                priority = "medium"
        
        # Cas 2: Apr√®s cr√©ation de features
        elif self.current_agent == "backlog":
            backlog_results = self._extract_backlog_results()
            
            if backlog_results.get("features_created"):
                feature_count = len(backlog_results.get("features_created", []))
                suggestions = [
                    f"‚úçÔ∏è G√©n√©rer les User Stories pour ces {feature_count} features",
                    "üé® Ajouter des crit√®res d'acceptation",
                    "üìä Consulter le backlog complet"
                ]
                context = f"{feature_count} feature{'s' if feature_count > 1 else ''} ajout√©e{'s' if feature_count > 1 else ''} au backlog"
                priority = "high"
            
            elif backlog_results.get("user_stories_created"):
                us_count = len(backlog_results.get("user_stories_created", []))
                suggestions = [
                    f"üß™ G√©n√©rer les Use Cases pour ces {us_count} User Stories",
                    "üìä Estimer les story points",
                    "üöÄ Planifier le prochain Sprint"
                ]
                context = f"{us_count} User Stor{'ies' if us_count > 1 else 'y'} pr√™te{'s' if us_count > 1 else ''}"
                priority = "high"
            
            else:
                suggestions = [
                    "üÜï Cr√©er un Epic structurant",
                    "üìö Extraire depuis vos documents",
                    "üëÄ Explorer le backlog"
                ]
                context = "Backlog vide - structurons votre projet"
                priority = "medium"
        
        # Cas 3: Erreur d√©tect√©e
        elif self.error:
            suggestions = [
                "üîÑ R√©essayer l'op√©ration",
                "üí¨ Obtenir de l'aide",
                "üè† Retour √† l'accueil"
            ]
            context = f"‚ö†Ô∏è Erreur: {self.error[:80]}"
            priority = "high"
        
        # Cas 4: D√©but de session (pas encore d'agent ex√©cut√©)
        else:
            suggestions = [
                "üìö Analyser vos documents",
                "üÜï Cr√©er un Epic ou une Feature",
                "üí¨ Expliquer votre besoin"
            ]
            context = "Bienvenue ! Comment puis-je vous aider ?"
            priority = "medium"
        
        return {
            "suggestions": suggestions,
            "context": context,
            "priority": priority,
            "emoji": self._get_context_emoji(priority)
        }
    
    def format_response(self, data: dict) -> str:
        """Formater une r√©ponse de mani√®re naturelle et engageante.
        
        Args:
            data: Donn√©es brutes √† formater
            
        Returns:
            str: R√©ponse format√©e en markdown avec emojis
        """
        logger.info("ConversationAgent formatting response")
        
        # En-t√™te bas√© sur le succ√®s
        if data.get("error"):
            header = "‚ùå **Oups, quelque chose n'a pas fonctionn√©**\n\n"
        elif data.get("success", True):
            header = "‚úÖ **Op√©ration termin√©e !**\n\n"
        else:
            header = "‚ÑπÔ∏è **Voici ce que j'ai trouv√©**\n\n"
        
        # Corps du message
        body_parts = []
        
        # Features cr√©√©es
        if data.get("features_created"):
            count = len(data["features_created"])
            body_parts.append(f"üéâ **{count} feature{'s' if count > 1 else ''}** cr√©√©e{'s' if count > 1 else ''} dans votre backlog")
            
            # Liste les IDs si peu nombreux
            if count <= 5:
                ids = ", ".join([f"#{id}" for id in data["features_created"]])
                body_parts.append(f"_IDs: {ids}_")
        
        # Documents trait√©s
        if data.get("documents_analyzed"):
            docs = data["documents_analyzed"]
            if isinstance(docs, list):
                doc_list = ', '.join([f"**{doc}**" for doc in docs[:3]])
                body_parts.append(f"üìÑ Documents analys√©s: {doc_list}")
                if len(docs) > 3:
                    body_parts.append(f"_et {len(docs) - 3} autre{'s' if len(docs) - 3 > 1 else ''}_")
        
        # R√©sultats de recherche
        if data.get("results"):
            results_count = len(data["results"])
            body_parts.append(f"üîç **{results_count} r√©sultat{'s' if results_count > 1 else ''}** pertinent{'s' if results_count > 1 else ''}")
        
        # Erreur
        if data.get("error"):
            error_msg = str(data["error"])[:200]
            body_parts.append(f"```\n{error_msg}\n```")
        
        # Message personnalis√©
        if data.get("message"):
            body_parts.append(data["message"])
        
        # Assemblage avec espacement am√©lior√©
        if not body_parts:
            body = "Op√©ration termin√©e avec succ√®s."
        else:
            body = "\n".join(body_parts)
        
        return header + body
    
    async def ask_clarification(self, ambiguity: str) -> str:
        """Poser des questions de clarification quand l'objectif est ambigu.
        
        Args:
            ambiguity: Description de l'ambigu√Øt√© d√©tect√©e
            
        Returns:
            str: Question de clarification format√©e
        """
        logger.info(f"ConversationAgent asking clarification for: {ambiguity}")
        
        # Utiliser le LLM pour g√©n√©rer une question contextuelle
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)
        
        prompt = f"""Tu es un assistant proactif qui aide les utilisateurs √† clarifier leurs demandes.

L'utilisateur a dit: "{self.state.get('objective', '')}"

Ambigu√Øt√© d√©tect√©e: {ambiguity}

G√©n√®re UNE question de clarification concise et naturelle pour r√©soudre cette ambigu√Øt√©.
La question doit:
- √ätre courte (max 2 phrases)
- Proposer 2-3 options concr√®tes si pertinent
- Utiliser un emoji pertinent au d√©but
- √ätre en fran√ßais

Exemples:
- "ü§î Je vois plusieurs fa√ßons de faire. Voulez-vous cr√©er un nouvel Epic ou ajouter √† un existant ?"
- "üìã D'accord ! Pour quelle feature voulez-vous g√©n√©rer les User Stories ?"

G√©n√®re uniquement la question, sans pr√©ambule."""

        messages = [
            SystemMessage(content=prompt)
        ]
        
        response = await llm.ainvoke(messages)
        question = response.content.strip()
        
        return question
    
    def _extract_document_results(self) -> dict:
        """Extraire les r√©sultats li√©s aux documents."""
        results = {}
        
        # Chercher dans tool_results
        if "draft_features_from_documents" in self.tool_results:
            result = self.tool_results["draft_features_from_documents"]
            if isinstance(result, dict):
                results["features_created"] = result.get("features_created", [])
                results["documents_analyzed"] = result.get("source_documents", [])
        
        if "list_documents" in self.tool_results:
            result = self.tool_results["list_documents"]
            if isinstance(result, dict):
                results["documents_list"] = result.get("documents", [])
        
        # Chercher dans state.documents_searched
        if self.state.get("documents_searched"):
            results["documents_analyzed"] = self.state["documents_searched"]
        
        return results
    
    def _extract_backlog_results(self) -> dict:
        """Extraire les r√©sultats li√©s au backlog."""
        results = {}
        
        # Chercher dans tool_results
        if "bulk_create_features" in self.tool_results:
            result = self.tool_results["bulk_create_features"]
            if isinstance(result, dict):
                results["features_created"] = result.get("features_created", [])
        
        if "generate_children_items" in self.tool_results:
            result = self.tool_results["generate_children_items"]
            if isinstance(result, dict):
                items = result.get("items_created", [])
                # D√©terminer le type
                if items and isinstance(items[0], dict):
                    item_type = items[0].get("type", "")
                    if item_type == "US":
                        results["user_stories_created"] = [i.get("id") for i in items]
                    elif item_type == "UC":
                        results["use_cases_created"] = [i.get("id") for i in items]
        
        # Chercher dans state.items_created
        if self.state.get("items_created"):
            results["items_created"] = self.state["items_created"]
        
        return results
    
    def _get_context_emoji(self, priority: str) -> str:
        """Retourner un emoji bas√© sur la priorit√©."""
        emoji_map = {
            "high": "üî•",
            "medium": "üí°",
            "low": "üí≠"
        }
        return emoji_map.get(priority, "üí¨")


async def conversation_agent_node(state: AgentState) -> AgentState:
    """Node LangGraph pour l'agent conversationnel."""
    logger.info(f"ConversationAgent processing: {state['objective']}")
    
    # Get streaming manager for this run
    run_id = state.get("run_id", "default")
    stream_manager = get_stream_manager(run_id)
    
    try:
        # Emit agent start event with workflow context if available
        workflow_context = state.get("workflow_context")
        await stream_manager.emit_agent_start(
            "conversation", 
            state["objective"], 
            state["iteration"],
            step_info=workflow_context
        )
        
        # Emit initial narration
        await stream_manager.emit_agent_narration(
            "conversation",
            "Je pr√©pare mes recommandations pour la suite",
            state["iteration"]
        )
        
        # Cr√©er l'agent
        agent = ConversationAgent(state)
        
        # D√©terminer l'action √† prendre
        objective_lower = state["objective"].lower()
        
        # Action: Sugg√©rer les prochaines √©tapes
        if any(keyword in objective_lower for keyword in ["suggest", "next", "what now", "maintenant", "ensuite", "quoi faire"]):
            logger.info("ConversationAgent: Suggesting next steps")
            
            suggestions = await agent.suggest_next_steps()
            
            # Emit narration with suggestions
            await stream_manager.emit_agent_narration(
                "conversation",
                "Voici ce que vous pouvez faire maintenant",
                state["iteration"]
            )
            
            # Formater la r√©ponse avec markdown am√©lior√©
            response_text = f"{suggestions['emoji']} **{suggestions['context']}**\n\n"
            response_text += "üëâ **Que souhaitez-vous faire ?**\n\n"
            for suggestion in suggestions['suggestions']:
                response_text += f"- {suggestion}\n"
            
            await stream_manager.emit_agent_end(
                "conversation",
                response_text,
                state["iteration"],
                success=True,
                extra_data={"suggestions": suggestions['suggestions']}
            )
            
            return {
                **state,
                "messages": state["messages"],
                "iteration": state["iteration"] + 1,
                "current_agent": "conversation",
                "final_response": response_text,
                "tool_results": {**state["tool_results"], "suggestions": suggestions}
            }
        
        # Action: Demander clarification
        elif any(keyword in objective_lower for keyword in ["clarify", "unclear", "ambigu", "pr√©ciser"]):
            logger.info("ConversationAgent: Asking for clarification")
            
            ambiguity = state.get("error", "L'objectif n'est pas clair")
            question = await agent.ask_clarification(ambiguity)
            
            await stream_manager.emit_agent_end(
                "conversation",
                question,
                state["iteration"],
                success=True
            )
            
            return {
                **state,
                "messages": state["messages"],
                "iteration": state["iteration"] + 1,
                "current_agent": "conversation",
                "final_response": question
            }
        
        # Par d√©faut: Formater la derni√®re r√©ponse
        else:
            logger.info("ConversationAgent: Formatting response")
            
            # R√©cup√©rer les r√©sultats du dernier agent
            last_results = state.get("tool_results", {})
            
            formatted = agent.format_response(last_results)
            
            # Ajouter automatiquement des suggestions
            suggestions = await agent.suggest_next_steps()
            
            # Emit narration with suggestions
            await stream_manager.emit_agent_narration(
                "conversation",
                "Voici ce que vous pouvez faire maintenant",
                state["iteration"]
            )
            
            formatted += f"\n\n{suggestions['emoji']} **Que souhaitez-vous faire maintenant ?**\n\n"
            for suggestion in suggestions['suggestions']:
                formatted += f"- {suggestion}\n"
            
            await stream_manager.emit_agent_end(
                "conversation",
                formatted,
                state["iteration"],
                success=True,
                extra_data={"suggestions": suggestions['suggestions']}
            )
            
            return {
                **state,
                "messages": state["messages"],
                "iteration": state["iteration"] + 1,
                "current_agent": "conversation",
                "final_response": formatted
            }
    
    except Exception as e:
        logger.error(f"ConversationAgent failed: {e}", exc_info=True)
        
        # Emit error
        await stream_manager.emit_agent_end(
            "conversation",
            f"ConversationAgent error: {str(e)}",
            state["iteration"],
            success=False
        )
        
        return {
            **state,
            "iteration": state["iteration"] + 1,
            "current_agent": "conversation",
            "error": f"ConversationAgent error: {str(e)}"
        }
